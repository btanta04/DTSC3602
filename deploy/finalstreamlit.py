import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
from pathlib import Path

# =====================================================
# PATHS (LOCAL + MODAL FRIENDLY)
# =====================================================

# Directory where this script lives (works locally)
LOCAL_BASE = Path(__file__).resolve().parent

# If running on Modal, files are under /root
if Path("/root").exists():
    BASE = Path("/root")
else:
    BASE = LOCAL_BASE

EMBED_PARQUET = BASE / "embeddings" / "outseer_articles_with_embeddings.parquet"
RAW_CSV = BASE / "outseer_articles.csv"
KW_CSV = BASE / "bertopic_keywords_weights.csv"


# =====================================================
# LOAD DATA
# =====================================================

@st.cache_data
def load_articles():
    """
    Load Outseer articles, preferring the parquet file that already
    contains sentence-transformer embeddings (emb_0, emb_1, ...).
    """
    if EMBED_PARQUET.exists():
        df = pd.read_parquet(EMBED_PARQUET)
    else:
        # Fallback if parquet doesn't exist yet
        df = pd.read_csv(RAW_CSV)

    # Remove first row (overview row) like your original
    df = df.iloc[1:].reset_index(drop=True)

    # Parse dates into a unified 'published' column
    if "published" in df.columns:
        df["published"] = pd.to_datetime(df["published"], errors="coerce")
    elif "date" in df.columns:
        df["published"] = pd.to_datetime(df["date"], errors="coerce")
    else:
        df["published"] = pd.NaT

    return df


@st.cache_data
def load_keywords():
    """
    Load BERTopic fraud-only keyword weights.
    Generated by bertopic script â†’ bertopic_keywords_weights.csv
    """
    if KW_CSV.exists():
        kw_df = pd.read_csv(KW_CSV)
        # Make sure expected columns exist
        expected_cols = {"keyword", "weight"}
        missing = expected_cols - set(kw_df.columns)
        if missing:
            st.warning(f"Keyword file missing columns: {missing}")
        return kw_df
    else:
        st.warning("bertopic_keywords_weights.csv not found. "
                   "Top keyword chart will be empty.")
        return pd.DataFrame(columns=["keyword", "weight"])


df = load_articles()
kw_df = load_keywords()

# =====================================================
# EMBEDDING + FRAUD SCORE LOGIC (USING PRECOMPUTED EMBEDDINGS)
# =====================================================

# Find embedding columns (created by your embeddings script)
EMB_COLS = [c for c in df.columns if c.startswith("emb_")]

if not EMB_COLS:
    st.warning(
        "No embedding columns (emb_0, emb_1, ...) found. "
        "Fraud scores will not be computed. "
        "Make sure you ran the embeddings script and "
        "are loading embeddings/outseer_articles_with_embeddings.parquet."
    )
    df["fraud_score"] = None
else:
    # --------- build a unified text field for fraud detection ---------
    text_cols = [
        c for c in ["summary", "full_text", "doc", "text_for_embedding"]
        if c in df.columns
    ]
    if text_cols:
        combined_text = (
            df[text_cols]
            .fillna("")
            .agg(" ".join, axis=1)
            .str.lower()
        )
    else:
        combined_text = pd.Series([""] * len(df))

    # Articles that look "fraud-y" based on text
    fraud_mask = combined_text.str.contains("fraud", na=False)

    fraud_df = df[fraud_mask]

    if fraud_df.empty:
        st.warning(
            "No articles containing the word 'fraud' were found. "
            "Fraud reference embedding cannot be computed."
        )
        df["fraud_score"] = None
    else:
        # Build fraud reference embedding from precomputed emb_* columns
        fraud_embs = fraud_df[EMB_COLS].to_numpy(dtype=float)

        fraud_reference = fraud_embs.mean(axis=0)

        # Normalize reference vector
        norm = np.linalg.norm(fraud_reference)
        if norm == 0:
            fraud_reference = None
            st.warning("Fraud reference embedding has zero norm.")
            df["fraud_score"] = None
        else:
            fraud_reference = fraud_reference / norm

            # Compute fraud score for each article
            all_embs = df[EMB_COLS].to_numpy(dtype=float)

            # Cosine similarity with normalized fraud_reference
            # (embeddings were normalized when created)
            sims = all_embs @ fraud_reference  # dot product

            # Clip to [0, 1] and turn into %
            sims = np.clip(sims, 0, 1)
            df["fraud_score"] = np.round(sims * 100, 2)

# =====================================================
# SIDEBAR NAVIGATION
# =====================================================

st.sidebar.title("Navigation")
page = st.sidebar.radio(
    "",
    ["Overview", "Article Explorer"]
)

# =====================================================
# OVERVIEW PAGE
# =====================================================

if page == "Overview":
    st.title("Outseer Overview")

    # ----------------------------------------------
    # Top Keywords (from BERTopic fraud keywords)
    # ----------------------------------------------
    st.subheader("Top Fraud-Related Keywords")

    if not kw_df.empty and "keyword" in kw_df.columns and "weight" in kw_df.columns:
        # Aggregate weights across topics
        kw_agg = (
            kw_df.groupby("keyword")["weight"]
            .sum()
            .reset_index()
        )
        top_kw = kw_agg.sort_values("weight", ascending=False).head(20)

        chart_keywords = (
            alt.Chart(top_kw)
            .mark_bar()
            .encode(
                x=alt.X("weight:Q", title="Total Topic Weight"),
                y=alt.Y("keyword:N", sort="-x", title="Keyword"),
                tooltip=["keyword", "weight"]
            )
        )

        st.altair_chart(chart_keywords, use_container_width=True)
        st.caption(
            "Keywords extracted from BERTopic, filtered to fraud-related terms."
        )
    else:
        st.info("No BERTopic keyword data available.")

    # ----------------------------------------------
    # Fraud Score Distribution
    # ----------------------------------------------

    st.subheader("Fraud Likelihood Distribution")

    df_fraud = df[df["fraud_score"].notna()]

    if not df_fraud.empty:
        fraud_chart = (
            alt.Chart(df_fraud)
            .mark_bar()
            .encode(
                x=alt.X(
                    "fraud_score:Q",
                    bin=alt.Bin(maxbins=20),
                    title="Fraud Score"
                ),
                y=alt.Y("count()", title="Number of Articles"),
                tooltip=["count()"]
            )
        )
        st.altair_chart(fraud_chart, use_container_width=True)
    else:
        st.info("Fraud scores are not available.")

    # ----------------------------------------------
    # Articles Over Time
    # ----------------------------------------------

    st.subheader("Articles Published Over Time")

    df_time = df[df["published"].notna()]

    if not df_time.empty:
        time_chart = (
            alt.Chart(df_time)
            .mark_line(point=True)
            .encode(
                x=alt.X("published:T", title="Date"),
                y=alt.Y("count()", title="Number of Articles"),
                tooltip=["published:T", "count()"]
            )
        )
        st.altair_chart(time_chart, use_container_width=True)
    else:
        st.info("No valid publication dates found.")

# =====================================================
# ARTICLE EXPLORER PAGE
# =====================================================

elif page == "Article Explorer":
    st.title("Article Explorer")

    # Sidebar search
    st.sidebar.subheader("Search Articles")
    search_title = st.sidebar.text_input("Search by Title")

    if search_title:
        df_display = df[df["title"].str.contains(search_title, case=False, na=False)]
    else:
        df_display = df.copy()

    # Sort by published date
    df_display = df_display.sort_values("published", ascending=False)

    if df_display.empty:
        st.info("No articles match your search.")
    else:
        selected_title = st.selectbox("Choose an article", df_display["title"])
        article = df_display[df_display["title"] == selected_title].iloc[0]

        # ---------------------
        # DISPLAY ARTICLE
        # ---------------------

        st.header(article["title"])

        if "url" in article and pd.notna(article["url"]):
            st.markdown(f"[Read Full Article]({article['url']})")

        if pd.notna(article.get("published", pd.NaT)):
            st.write(f"Published: {article['published'].date()}")

        st.subheader("Summary")
        if "summary" in article:
            st.write(article["summary"])
        else:
            st.write("No summary available.")

        st.subheader("Full Text")
        with st.expander("Show full article text"):
            if "full_text" in article:
                st.write(article["full_text"])
            else:
                st.write("No full text available.")

        st.subheader("Fraud Likelihood")
        score = article.get("fraud_score", None)
        if score is None or (isinstance(score, float) and np.isnan(score)):
            st.info("Fraud reference embedding not available.")
        else:
            st.metric("Fraud Score", f"{score}%")
            st.progress(float(score) / 100.0)